{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b63a5b37-7193-4181-be5f-3973fd224678",
   "metadata": {},
   "source": [
    "# Data Science Meets Government of Canada Web Services\n",
    "\n",
    "***By Anita Li, Deepak Sidhu, Jianru Deng, Sakshi Jain***<br>\n",
    "*June 28th, 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190956d",
   "metadata": {},
   "source": [
    "## Final Report\n",
    "\n",
    "### 1. Executive Summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28df9d",
   "metadata": {},
   "source": [
    "The Web Analytics team from the Government of Canada (GoC) is seeking advice on use cases of data science techniques that can be added to their current Adobe Analytics (AA) workflow. To tackle their questions, we demonstrate the application of: **time series forecasting, prediction on service satisfaction, survey topic modeling**, and provide overviews of **chatbot, A/B testing, and recommender system** techniques. Reproducible approach is also developed by implementing **python wrapper** and **Jupyter Book**. These use cases show the benefits and value-added of employing data science techniques other than those offered by AA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60648600",
   "metadata": {},
   "source": [
    "### 2. Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d422574",
   "metadata": {},
   "source": [
    "The Web Analytics team leverages AA to understand and analyze web traffic data. Although AA is a powerful tool that manages a large and complex dataset (timestamped web-traffic and personas for all traffic to GoC web properties), it has limitations in terms of data science applications. For example, AA is not capable of implementing advanced machine learning models to analyze online survey data. To get useful insights from the web data and provide better services to visitors, the Web Analytics team needs advanced data science techniques, and most of them are hard to implement by AA alone. \n",
    "The project objective is to demonstrate the advantages of data science using multiple techniques to achieve higher user satisfaction and task completion rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0548a5",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Data Product and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca0d24",
   "metadata": {},
   "source": [
    "The data product for the capstone project is a [Jupyter Book](https://ubc-mds.github.io/Capstone_CanadaWebAnalysis/) that provides reproducible workflow and documentation of the use cases of data science techniques applicable to web analytics data. The GoC team will use the Jupyter Book (Fig. 1) as a guide on how to implement data science techniques to gain useful insights and knowledge to inform decision-making processes. Also, the Jupyter Book will emphasize the importance of reproducibility and how this streamlines the workflow process within the team. It is important to make reproducibility an important aspect considered in their workflow processes, since other government organizations have shifted to reproducibility to ensure integrity of results. The Jupyter Book will also provide the foundation and framework to communicate to managers about the importance of incorporating data science.\n",
    " \n",
    "```{figure} img/jupyter-book_sample.png \n",
    "---\n",
    "name: jupyter-book\n",
    "---\n",
    "Example page from Jupyter Book data product\n",
    "```\n",
    "\n",
    "Jupyter Book is an open-source tool for producing quality, interactive documents based on computational material, and provides automated workflows for data analytics. The Jupyter Book provided here demonstrates the data science techniques applicable to web analytics data in Python {cite}`van1995python`, with relevant diagnostics and graphics. Developing Jupyter Book involves a steep learning curve, and it is specific to Python, R and Julia programmers. Effective collaboration is important if multiple individuals are working on the same project. Installing dependencies specific to particular operating system are an issue, hence the capstone team has provided an environment file to reproduce the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02f21",
   "metadata": {},
   "source": [
    "### 4. Data Science Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b312a7",
   "metadata": {},
   "source": [
    "#### 4.1 Time Series Forecasting\n",
    "\n",
    "Time series techniques predict the future values of a process within confidence bounds using a computational model. These techniques can be applied to a number of questions within the Web Services domain, ranging from web traffic forecasting to conversion predictions of users. Also, the time series forecasting can be used with other data science techniques mentioned in this report. This provides additional benefits such as implementing surveys on the pages of the website where traffic prediction is significantly high. \n",
    "\n",
    "\n",
    "```{figure} img/visitors-trend.PNG\n",
    "---\n",
    "\n",
    "name: visitors-trend\n",
    "---\n",
    "Daily Web Traffic of unique visotrs coming to the Government of Canada website.\n",
    "```\n",
    "\n",
    "There are many time series techniques available. However, we are going to focus on the **Holt-Winters’ method, SARIMA, Recursive method** and **Prophet**{cite}`fbprophet`, as these methods focus on trend and seasonality. Prior knowledge of trend/seasonality helps in tuning the model to get accurate prediction. From the Fig. 2, the trend rises from a level of 1 million on January 1st, 2019 to the level of 4 million on April 30th, 2021. Also, we have a strong periodicity as traffic goes down every weekend and comes up again every Monday. By using all 4 techniques, more than 100 models were created and the technique Prophet was selected to forecast traffic. Below is the description of methods used in predicting unique visitors.\n",
    "\n",
    "**Holt-Winters' method**: Holt-Winter is also known as a linear exponential smoothing method. This model takes into account the data’s seasonality, and utilizes exponential smoothing to give more weight to more recent historical data. \n",
    "\n",
    "**Seasonal AutoRegressive Integrated Moving Average (SARIMA)**{cite}`prophetbook`: SARIMA captures moving averages throughout time series, taking into account the data’s trend, seasonality, and noise. This model includes user-specified values to increase the ‘fine-tune ability’ of the model.\n",
    "\n",
    "**Machine learning recursive method (MLRM)**: This model is used to predict one-step ahead. The value obtained from the forecasting is then fed into the same model to predict the following step and so on until the desired forecasting horizon is reached. \n",
    "\n",
    "**Prophet**: A big difference between Prophet and the other mentioned models is that Prophet allows for multi-seasonality forecasting with daily, weekly, AND yearly (vs. OR), along with holiday detection. Prophet includes automatic detection for all seasonality/trend, and allows for user input for more precise fine-tuning. \n",
    "\n",
    "We used Prophet to predict the traffic on the entire website and its 2 sections:  **Revenue-agency** (the most visited section) and **Public-health** (Covid related pages). An interactive dashboard has been created to show the results, which can be accessed by [clicking here](https://forecastinguniquevisitors.herokuapp.com/) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4e332",
   "metadata": {},
   "source": [
    "#### 4.2 Prediction on Web Satisfaction \n",
    "\n",
    "A satisfaction/task survey is presented to 5% of website visitors. \n",

    "The Web Analytics team would like to use this information to extrapolate user satisfaction across all users to understand visitors' experience and satisfaction level broadly. Several supervised machine learning models {cite}`sml2018` have been applied to find their predicting performances. Moreover, the top ten important features are extracted in terms of reflecting and predicting visitor satisfaction. Finally, we have demonstrated a hyperparameter tuning process to better optimize the predicting performance.\n",
    "\n",
    "```{figure} img/pred_feats.png \n",
    "---\n",
    "name: prediction-features\n",
    "---\n",
    "Sample data and features have been used in predicting on satisfaction.\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} img/pred_res.png \n",
    "---\n",
    "name: prediction-results\n",
    "---\n",
    "Model evaluation on their fitting time, accuracy and weighted f1 score.\n",
    "```\n",
    "\n",
    "\n",
    "After rounds of feature selection and data preprocessing, only seven features have been fitted (Fig. 3). From the Fig. 4 of model evaluation, **MLP (Multi-layer Perceptron) and Random Forest** {cite}`mlp`are ranked as the top two best models, based on their relatively high value on accuracy and weighted f1 score. It is also noticeable that overfitting is not observed at this stage. \n",
    "All those 7 models predicted better than the baseline Dummy model, which suggests improving performance is possible with larger datasets. \n",
    "As a trade-off, MLP and Random Forest are also time-consuming in their model fitting time, which could potentially be solved by applying GPU architecture.\n",
    "Another drawback of this modelling process is that the number of features is still quite limited, and each data point is simplified as one entry. As a consequence, the connection of page referral is not considered in modelling. \n",
    "Ideally if the trajectory of a visitor's browsing history is traced and fitted in a model {cite}`dl2020`, his/her behavior will be even better understood.\n",
    "\n",
    "Looking forward, for the next phase of this particular task, a larger volume of data should be analyzed, given this demonstration of only one week of data. \n",
    "Moreover, the time-series analysis could be another factor, since events or some particular circumstances (e.g., COVID) may influence visitors' expectation {cite}`covid2020`. \n",
    "Understanding the sequential order of the browsing history would leverage the value of AA. If the sequential data are well cleaned and processed, the analytics team could even predict the next possible page that a visitor would click. However, data cleaning and preprocessing techniques for sequence modelling can be very challenging, since the data collection and ingestion workflow in AA makes it hard to maintain sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b082a",
   "metadata": {},
   "source": [
    "#### 4.3 Survey Feedback Topic Modeling\n",
    "\n",
    "The GoC web survey includes the “open” text question: *Why were you not able to complete what you came to do?* Modeling and identifying major topics from the feedback responses will provide emerging topics and issues encountered by visitors.\n",
    "\n",
    "Topic modeling using **LDA (Latent Dirichlet Allocation)** {cite}`944937` and **GSDMM (Gibbs Sampling Dirichlet Mixture Model)** {cite}`yin2014dirichlet`, and clustering with **K-means** {cite}`na2010research` were implemented to model the text feedback data. LDA and GSDMM return topic groups based on how well an individual text entry fits into the distribution of words that make up the predicted topic clusters. LDA is appropriate for lengthier text documents that are composed of multiple topics, and GSDMM is suitable for text documents based on a single topic. In K-means clustering, we assume all survey responses fill an n-dimensional space, where n is the total number of unique words across all survey responses. Clustering is then based on similarity between individual observations in the geometric space.\n",
    "\n",
    "As expected, the GSDMM model can identify finer topic clusters compared to the other models. Fig. 5 diaplays the results of the different models topic assignment on the test feedback.\n",
    "   \n",
    "```{figure} img/feedback-results.png \n",
    "---\n",
    "width: 700px\n",
    "height: 350px\n",
    "name: feedback-examples\n",
    "---\n",
    "Example of test text feedback and topic assignments by the models\n",
    "```\n",
    "\n",
    "There are areas of improvement for optimizing the analysis of text feedback data. First, we only have 3,181 feedback data, which is quite limited in training a sophisticated machine learning model. Second, it is crucial to improve the preprocessing of the text to include spell check, and anomaly detection to ensure accurate responses to the question. Third, we don’t have ground truth labels for objectively evaluating the topic assignments of the models. Hence, it is helpful to train a supervised machine learning model on text feedback data and topic labels assigned by the GSDMM model, so that we can evaluate model performance and automatically assign topics for future feedback responses.\n",
    "\n",
    "Some of the difficulties encountered includes assigning values to the hyperparameters **k**,  **alpha** and **beta** of the GSDMM model; where k refers to number of topic clusters, alpha is the affinity toward groups with the same size, and beta is the affinity toward groups with similar terms. Also in terms of clustering techniques, it is important to consider density-based methods such as **DBSCAN** {cite}`schubert2017dbscan` and **HDBSCAN** {cite}`McInnes2017`, which don’t require specifying the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4662cb0-3d44-458d-8d33-e59092406eb1",
   "metadata": {},
   "source": [
    "#### 4.4 Further Data Science Techniques\n",
    "\n",
    "##### Chatbot\n",
    "Chatbot {cite}`chatbot2016` is an implementation of a deep learning model that associates certain responses with unstructured user statements. Chatbots can be efficient since the program runs 24/7 and respond to multiple visitors simultaneously. Chatbots also collect large amounts of unstructured text data during conversations that will be used to inform other techniques such as topic modeling. Challenges and concerns of implementing Chatbot include: 1)  trade-off between budget and performance of chatbot, 2) concerns of collecting personal information, and 3) accessibility settings of chatbots.\n",
    "\n",
    "##### A/B testing\n",
    "A/B testing is a statistical hypothesis testing used to compare two versions of a single variable. In the web analytics field, A/B testing is widely used to understand users’ responses to online features {cite}`abtesting`, such as a new feature or product. With the help of A/B testing and user's behavior data, decision makers are able to know if certain changes are linked to any specific features. To implement A/B testing successfully, it requires sufficient statistical knowledge to design the experiment comprehensively. It should avoid any changes to the underlying data/web pages to ensure the experiment consistency.\n",
    "\n",
    "##### Recommender system\n",
    "Recommender system is developed to recommend certain content to web users. Two approaches to recommendation systems are collaborative filtering and content-based filtering {cite}`recomend2012`, which are usually combined into hybrid systems to develop advanced models. To implement the system successfully, the Web Analytics team needs to be prepared for extensive machine learning and data mining, and it may take time to collect enough data for training purposes. It may also be challenging to determine the right metrics of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22293ae7",
   "metadata": {},
   "source": [
    "### 5. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4131360",
   "metadata": {},
   "source": [
    "The data product of Jupyter Book addresses the question, *What are the use cases of data science techniques for the GoC’s web analytics data?* Also, the [Jupyter Book](https://ubc-mds.github.io/Capstone_CanadaWebAnalysis/) provides reproducible workflow and detailed documentation of key data science techniques applicable to web analytics data. Also, it includes discussion and resources of additional data science topics applicable to the web data. The Jupyter Book addresses the needs of the GoC’s Web Analytics team, since it provides collective strategies of data science subfields that can be implemented to transform the processes of the Web Analytics team. Furthermore, this Book also emphasizes reproducibility as a core component of their analysis pipeline.\n",
    "\n",
    "The recommendation for the Web Analytics team to improve the project in the future is to implement other data science techniques mentioned in the Jupyter Book. Techniques such as an accessible chatbot, visitor segmentation, A/B testing, recommender system will improve and optimize the GoC's web services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb00f63",
   "metadata": {},
   "source": [
    "### 6. Bibliography\n",
    "\n",
    "```{bibliography} references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
