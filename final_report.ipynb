{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcf5731",
   "metadata": {},
   "source": [
    "# Data Science Meets Government of Canada Web Services\n",
    "\n",
    "***By Anita Li, Deepak Sidhu, Jianru Deng, Sakshi Jain***<br>\n",
    "*June 28th, 2021*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190956d",
   "metadata": {},
   "source": [
    "## Final Report\n",
    "\n",
    "### 1. Executive Summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28df9d",
   "metadata": {},
   "source": [
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60648600",
   "metadata": {},
   "source": [
    "### 2. Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d422574",
   "metadata": {},
   "source": [
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0548a5",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Data Product and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca0d24",
   "metadata": {},
   "source": [
    "The data product for the capstone project is a Jupyter Book that provides reproducible workflow and documentation of the use cases of data science techniques applicable to web analytics data. The GoC team will use the Jupyter Book as a guide on how to implement data science techniques to gain useful insights and knowledge to inform business decisions and processes. Also, the Jupyter Book will emphasize the importance of reproducibility and how this streamlines the workflow process within the team. Furthermore, it is quite important to make reproducibility an important aspect considered in their workflow processes. Since other organizations have shifted to  reproducibility to ensure reliability and integrity of results. More importantly, it will also provide the foundation and framework to communicate to managers the importance of incorporating data science as an approach in the processes of GoC.\n",
    " \n",
    "```{figure} img/jupyter-book_sample.png \n",
    "---\n",
    "name: jupyter-book\n",
    "---\n",
    "Example page from Jupyter Book data product\n",
    "```\n",
    "\n",
    "Jupyter Book is an open-source tool for producing quality, interactive documents based on computational material and data science techniques. Jupyter books provides  clear and automated workflows for data analytics. The Jupyter Book provided here demonstrates the data science techniques applicable to web analytics data in Python {cite}`van1995python`, with relevant diagnostics and graphics. Developing Jupyter Book involves a steep learning curve, and it is specific to Python, R and Julia programmers. Effective collaboration is important if multiple individuals are working on the same project. The dependencies and requirements specific to the operating system are an issue, hence the capstone team has provided an environment file to reproduce the results in building the Jupyter book. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02f21",
   "metadata": {},
   "source": [
    "### 4. Data Science Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b312a7",
   "metadata": {},
   "source": [
    "#### 4.1 Time Series Forecasting\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4e332",
   "metadata": {},
   "source": [
    "#### 4.2 Prediction on Web Satisfaction \n",
    "\n",
    "A satisfaction/task survey is presented to 5% of website visitors.  The Web Services team would like to use this information to extrapolate user satisfaction across all users to understand visitors' experience and satisfaction level broadly. Several supervised machine learning models have been applied to test on their performances in prediction, with extracting the top ten important features, as well as a demonstration of hyperparameter tuning process.\n",
    "\n",
    "After realizing the difficulties of incorporating text features into the model, only seven features have been fitted. From the model evaluation section, MLP and Random Forest are ranked as the top two best models, based on their relatively high score on accuracy and weighted f1 score. It is also noticeable that overfitting is not observed at this stage. All those 7 models predict better than the baseline Dummy model, which are promising into the next phase to incorporate an even larger volume of dataset. As a trade-off, MLP and Random Forest are also time-consuming in their model fitting time, which could be potentially solved when applying GPU architecture in neural networks modelling for faster performance. Another drawback of this modelling process is that the amount of features are still quite limited, and each data point is simplified as one entry, in which the connection of page referral is not considered in modelling. Ideally if the trajectory of a visitor's browsing history is traced and fitted in a model, his behavior will be even better understood.\n",
    "\n",
    "Looking forward, for the next phase of this particular task, a larger volume of data should be analyzed, given this demonstration of only one week of browsing history. Imaging at least three months of visiting history will be more informative to predict visitors' satisfaction level. The time-series analysis may be informed by these findings, since events or some particular circumstances (e.g., COVID) could affect visitors' expectation as well as satisfaction. Some promising research ideas on modelling methods could be LSTM modelling. This is because the browsing history can be ephemeral and treated as a sequence of data instead of being static. Understanding the sequential order of the browsing history would leverage the value of AA. If the sequential data are well cleaned and processed, the analytics team could even predict the next possible page a visitor would click. However, data cleaning and preprocessing techniques for this modelling can be very challenging, since the data collection and ingestion approach in AA is hard to extract and store sequential data, plus it will triggle more challenges in feature engineering as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b082a",
   "metadata": {},
   "source": [
    "#### 4.3 Survey Feedback Topic Modeling\n",
    "\n",
    "\n",
    "The GoC web survey includes the “open” text question: *Why were you not able to complete what you came to do?* Modeling and identifying major topics from the feedback responses will provide crucial insights in improving and optimizing the delivery of web services.\n",
    "\n",
    "Topic modeling using **LDA (Latent Dirichlet Allocation)** {cite}`944937` and **GSDMM (Gibbs Sampling Dirichlet Mixture Model)** {cite}`yin2014dirichlet`, and clustering with **K-means** {cite}`na2010research` were implemented to model the text feedback data. LDA and GSDMM return topic groups based on how well an individual text entry fits into the distribution of words that make up the predicted topic clusters. LDA is appropriate for lengthier text documents that are composed of multiple topics, and GSDMM is suitable for text documents based on a single topic. In K-means clustering, we assume all survey responses fill an n-dimensional space, where n is the total number of unique words across all survey responses. Clustering is then based on similarity between individual observations in the geometric space.\n",
    "\n",
    "As expected, the GSDMM model can identify more specific and finer topic clusters compared to the other models. We can see the results of the different models topic assignment on the test feedback below.\n",
    "   \n",
    "```{figure} img/feedback-results.png \n",
    "---\n",
    "width: 700px\n",
    "height: 350px\n",
    "name: feedback-examples\n",
    "---\n",
    "Example of test text feedback and topic assignments by the models\n",
    "```\n",
    "\n",
    "There are some areas of improvement for optimizing the analysis of text feedback data. First, we only have 3,181 feedback data, which is quite limited in training a sophisticated machine learning model. Second, it is crucial to improve the preprocessing of the text to include spell check, language, and anomaly detection to ensure accurate responses to the question being asked. Third, we don’t have ground truth labels for objectively evaluating the topic assignments of the models. Hence, it is helpful to train a supervised machine learning model on text feedback data and topic labels assigned by the GSDMM model, so that we can evaluate model performance and automatically assign topics for future feedback responses.\n",
    "\n",
    "Some of the difficulties encountered includes assigning values to the hyperparameters **k**,  **alpha** and **beta** of the GSDMM model.  Where k refers to number of topic clusters, alpha is the affinity toward groups with the same size and beta is the affinity toward groups with similar terms. Also in terms of clustering techniques, it is important to consider density-based methods such as **DBSCAN** {cite}`schubert2017dbscan` and **HDBSCAN** {cite}`McInnes2017`, which don’t require specifying the number of topic clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22293ae7",
   "metadata": {},
   "source": [
    "### 5. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4131360",
   "metadata": {},
   "source": [
    "The data product of Jupyter Book addresses the question, *What are the use cases of data science techniques for the GoC’s web analytics data?* Also, the Jupyter Book  provides reproducible workflow and detailed documentation of key data science techniques applicable to web analytics data. Also, it includes discussion and resources of additional data science topics applicable to the web data. The Jupyter Book addresses the pressing needs of the GoC’s Web Services team since it provides collective strategies of data science subfields that can be implemented to transform the processes of the Web Services team, and it also emphasizes reproducibility as a core component for their analysis pipeline.\n",
    "\n",
    "The recommendation for the Web Services team to improve the project in the future is to implement other data science techniques mentioned in the Jupyter Book. Techniques such as accessible ChatBot, Visitor Segmentation, A/B testing, Recommendation system and Spell Checker will improve and optimize the web services provided by the GoC. Also, the Web Services team can use data science applications mentioned as collective strategies to gain crucial insights and aid decision-making processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb00f63",
   "metadata": {},
   "source": [
    "### 6. Bibliography\n",
    "\n",
    "```{bibliography} references.bib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11565ed-040f-4b40-a633-8f0665e6d857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
